{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"\"\"London is the capital and most populous city of England and the United Kingdom. Standing on \n",
    "the River Thames in the south east of the island of Great Britain, \n",
    "London has been a major settlement for two millennia. It was founded by the Romans, who named it Londinium.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['London is the capital and most populous city of England and the United Kingdom.', 'Standing on \\nthe River Thames in the south east of the island of Great Britain, \\nLondon has been a major settlement for two millennia.', 'It was founded by the Romans, who named it Londinium.']\n"
     ]
    }
   ],
   "source": [
    "sent=list(sent_tokenize(text))\n",
    "print(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Words Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words(l):\n",
    "    i=0\n",
    "    words=[]\n",
    "    for j in l:\n",
    "        words.append(word_tokenize(j))\n",
    "        i=i+1\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['London', 'is', 'the', 'capital', 'and', 'most', 'populous', 'city', 'of', 'England', 'and', 'the', 'United', 'Kingdom', '.'], ['Standing', 'on', 'the', 'River', 'Thames', 'in', 'the', 'south', 'east', 'of', 'the', 'island', 'of', 'Great', 'Britain', ',', 'London', 'has', 'been', 'a', 'major', 'settlement', 'for', 'two', 'millennia', '.'], ['It', 'was', 'founded', 'by', 'the', 'Romans', ',', 'who', 'named', 'it', 'Londinium', '.']]\n"
     ]
    }
   ],
   "source": [
    "word=words(sent)\n",
    "print(word)\n",
    "token=word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stop-Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stop_words(l):\n",
    "    stop=[]\n",
    "    sr = stopwords.words('english')\n",
    "    for i in range(len(l)):\n",
    "        sent=[]\n",
    "        for j in range(len(l[i])):\n",
    "            if l[i][j] not in sr:\n",
    "                sent.append(l[i][j])\n",
    "        stop.append(sent)\n",
    "    return stop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['London', 'capital', 'populous', 'city', 'England', 'United', 'Kingdom', '.'], ['Standing', 'River', 'Thames', 'south', 'east', 'island', 'Great', 'Britain', ',', 'London', 'major', 'settlement', 'two', 'millennia', '.'], ['It', 'founded', 'Romans', ',', 'named', 'Londinium', '.']]\n"
     ]
    }
   ],
   "source": [
    "stop=stop_words(token)\n",
    "print(stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part of Speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('London', 'NNP'), ('is', 'VBZ'), ('the', 'DT'), ('capital', 'NN'), ('and', 'CC'), ('most', 'RBS'), ('populous', 'JJ'), ('city', 'NN'), ('of', 'IN'), ('England', 'NNP'), ('and', 'CC'), ('the', 'DT'), ('United', 'NNP'), ('Kingdom', 'NNP'), ('.', '.')], [('Standing', 'VBG'), ('on', 'IN'), ('the', 'DT'), ('River', 'NNP'), ('Thames', 'NNP'), ('in', 'IN'), ('the', 'DT'), ('south', 'JJ'), ('east', 'NN'), ('of', 'IN'), ('the', 'DT'), ('island', 'NN'), ('of', 'IN'), ('Great', 'NNP'), ('Britain', 'NNP'), (',', ','), ('London', 'NNP'), ('has', 'VBZ'), ('been', 'VBN'), ('a', 'DT'), ('major', 'JJ'), ('settlement', 'NN'), ('for', 'IN'), ('two', 'CD'), ('millennia', 'NN'), ('.', '.')], [('It', 'PRP'), ('was', 'VBD'), ('founded', 'VBN'), ('by', 'IN'), ('the', 'DT'), ('Romans', 'NNPS'), (',', ','), ('who', 'WP'), ('named', 'VBD'), ('it', 'PRP'), ('Londinium', 'NNP'), ('.', '.')]]\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(word)):\n",
    "    word[i]=pos_tag(word[i])\n",
    "print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemminization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_stem(w):\n",
    "    return lancaster_stemmer.stem(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'words = [\"program\", \"programs\", \"programer\", \"programing\", \"programers\"] \\nps=PorterStemmer()\\nfor w in words: \\n    print(w, \" : \", ps.stem(w))'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"words = [\"program\", \"programs\", \"programer\", \"programing\", \"programers\"] \n",
    "ps=PorterStemmer()\n",
    "for w in words: \n",
    "    print(w, \" : \", ps.stem(w))\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps=PorterStemmer()\n",
    "def stem(l):\n",
    "    for i in range(len(l)):\n",
    "        for j in range(len(l[i])):\n",
    "            l[i][j]=stemmer.stem(l[i][j])\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemr=stem(stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['london', 'capit', 'pop', 'city', 'england', 'unit', 'kingdom', '.'], ['stand', 'riv', 'tham', 'sou', 'east', 'island', 'gre', 'britain', ',', 'london', 'maj', 'settl', 'two', 'millenn', '.'], ['it', 'found', 'rom', ',', 'nam', 'londin', '.']]\n"
     ]
    }
   ],
   "source": [
    "print(stemr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "list.remove(x): x not in list",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-43-5314d0888d66>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mstop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstop_words\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-39-ed59459565be>\u001b[0m in \u001b[0;36mstop_words\u001b[1;34m(l)\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m                 \u001b[0ml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: list.remove(x): x not in list"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
